{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820adc40",
   "metadata": {},
   "source": [
    "# Day 3: CNN Building Blocks\n",
    "## CV Bootcamp 2024\n",
    "\n",
    "Learn the fundamental components of Convolutional Neural Networks and how to calculate output dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775b3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d9bbcf",
   "metadata": {},
   "source": [
    "## 1. Convolutional Layers\n",
    "\n",
    "The core of CNNs - applies filters across images to detect features.\n",
    "\n",
    "**Key Concept:** A filter (kernel) slides across the image, computing dot products at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single convolutional layer\n",
    "conv = nn.Conv2d(\n",
    "    in_channels=3,      # RGB input\n",
    "    out_channels=32,    # 32 filters\n",
    "    kernel_size=3,      # 3x3 filters\n",
    "    stride=1,           # Move 1 pixel at a time\n",
    "    padding=1           # Pad to keep size same\n",
    ")\n",
    "\n",
    "# Input: batch of RGB images\n",
    "x = torch.randn(16, 3, 32, 32)  # (batch, channels, height, width)\n",
    "output = conv(x)\n",
    "\n",
    "print(f'Input shape: {x.shape}')\n",
    "print(f'Output shape: {output.shape}')  # (16, 32, 32, 32) - 32 feature maps\n",
    "print(f'Number of learnable parameters: {sum(p.numel() for p in conv.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "padding_section",
   "metadata": {},
   "source": [
    "## 2. Understanding Padding\n",
    "\n",
    "### The Problem: Convolution Shrinks Images!\n",
    "\n",
    "Without padding, each convolution reduces the spatial dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "padding_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate shrinking without padding\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# No padding\n",
    "conv_no_pad = nn.Conv2d(3, 32, kernel_size=3, padding=0)\n",
    "out_no_pad = conv_no_pad(x)\n",
    "\n",
    "# With padding\n",
    "conv_with_pad = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "out_with_pad = conv_with_pad(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output without padding:\", out_no_pad.shape)  # Shrinks to 30x30\n",
    "print(\"Output with padding=1:\", out_with_pad.shape)  # Stays 32x32\n",
    "\n",
    "# After 5 layers without padding\n",
    "print(\"\\nAfter 5 conv layers without padding:\")\n",
    "temp = x\n",
    "for i in range(5):\n",
    "    temp = conv_no_pad(temp)\n",
    "    print(f\"  After layer {i+1}: {temp.shape[2]}x{temp.shape[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output_calc",
   "metadata": {},
   "source": [
    "## 3. Output Size Calculation\n",
    "\n",
    "### Formula:\n",
    "$$\\text{Output Size} = \\left\\lfloor \\frac{\\text{Input} - \\text{Kernel} + 2 \\times \\text{Padding}}{\\text{Stride}} \\right\\rfloor + 1$$\n",
    "\n",
    "Let's verify this with examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calc_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_output_size(input_size, kernel_size, padding, stride):\n",
    "    \"\"\"Calculate output size for conv layer\"\"\"\n",
    "    return (input_size - kernel_size + 2 * padding) // stride + 1\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    (32, 3, 0, 1),   # No padding\n",
    "    (32, 3, 1, 1),   # Same padding\n",
    "    (32, 5, 2, 1),   # Larger kernel\n",
    "    (32, 3, 1, 2),   # Stride 2\n",
    "    (224, 7, 3, 2),  # ResNet first layer\n",
    "]\n",
    "\n",
    "print(\"Input | Kernel | Pad | Stride | Calculated | Actual\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for inp, ker, pad, stride in test_cases:\n",
    "    calculated = calc_output_size(inp, ker, pad, stride)\n",
    "    \n",
    "    # Verify with actual PyTorch\n",
    "    x = torch.randn(1, 3, inp, inp)\n",
    "    conv = nn.Conv2d(3, 16, kernel_size=ker, padding=pad, stride=stride)\n",
    "    actual = conv(x).shape[2]\n",
    "    \n",
    "    match = \"✓\" if calculated == actual else \"✗\"\n",
    "    print(f\"{inp:5} | {ker:6} | {pad:3} | {stride:6} | {calculated:10} | {actual:6} {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practice",
   "metadata": {},
   "source": [
    "### Practice Problems\n",
    "\n",
    "Calculate the output size for these scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practice_problems",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: Calculate these yourself first!\n",
    "problems = [\n",
    "    (128, 5, 2, 1),\n",
    "    (64, 3, 0, 2),\n",
    "    (224, 11, 5, 4),\n",
    "]\n",
    "\n",
    "print(\"Try calculating these:\")\n",
    "for i, (inp, ker, pad, stride) in enumerate(problems, 1):\n",
    "    print(f\"Problem {i}: Input={inp}, Kernel={ker}, Padding={pad}, Stride={stride}\")\n",
    "    print(f\"  Your answer: ___\")\n",
    "\n",
    "# Uncomment to see answers\n",
    "# print(\"\\nAnswers:\")\n",
    "# for i, (inp, ker, pad, stride) in enumerate(problems, 1):\n",
    "#     ans = calc_output_size(inp, ker, pad, stride)\n",
    "#     print(f\"Problem {i}: {ans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a915998",
   "metadata": {},
   "source": [
    "## 4. Activation Functions\n",
    "\n",
    "Add non-linearity to enable learning complex patterns.\n",
    "\n",
    "**Why?** Without activation functions, multiple layers would collapse to a single linear transformation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU: max(0, x) - most common\n",
    "relu = nn.ReLU()\n",
    "x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(f'Input: {x}')\n",
    "print(f'ReLU output: {relu(x)}')  # Negative values become 0\n",
    "\n",
    "# Visualize ReLU\n",
    "x_range = torch.linspace(-3, 3, 100)\n",
    "y_relu = relu(x_range)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x_range.numpy(), y_relu.numpy(), linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('ReLU Activation Function')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa456657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other activation functions\n",
    "sigmoid = nn.Sigmoid()\n",
    "tanh = nn.Tanh()\n",
    "leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "x = torch.tensor([-1.0, 0.0, 1.0])\n",
    "print(f'Input: {x}')\n",
    "print(f'Sigmoid: {sigmoid(x)}')\n",
    "print(f'Tanh: {tanh(x)}')\n",
    "print(f'Leaky ReLU: {leaky_relu(x)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7932597f",
   "metadata": {},
   "source": [
    "## 5. Pooling Layers\n",
    "\n",
    "Downsample feature maps to:\n",
    "- Reduce computation\n",
    "- Make features robust to small shifts\n",
    "- Increase receptive field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max pooling: take maximum value in each region\n",
    "max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "x = torch.randn(1, 32, 32, 32)\n",
    "output = max_pool(x)\n",
    "\n",
    "print(f'Input shape: {x.shape}')\n",
    "print(f'Output shape: {output.shape}')  # Halved dimensions\n",
    "\n",
    "# Demonstrate max pooling visually\n",
    "sample = torch.tensor([[\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12],\n",
    "    [13, 14, 15, 16]\n",
    "]], dtype=torch.float32).unsqueeze(0)  # Add batch and channel dims\n",
    "\n",
    "pooled = max_pool(sample)\n",
    "print(\"\\nBefore pooling:\")\n",
    "print(sample.squeeze())\n",
    "print(\"\\nAfter 2x2 max pooling:\")\n",
    "print(pooled.squeeze())\n",
    "print(\"\\nNotice: Each 2x2 region → maximum value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462910ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average pooling\n",
    "avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "output_avg = avg_pool(x)\n",
    "print(f'Avg pool output: {output_avg.shape}')\n",
    "\n",
    "# Compare max vs avg pooling\n",
    "pooled_avg = avg_pool(sample)\n",
    "print(\"\\nMax pooling result:\")\n",
    "print(pooled.squeeze())\n",
    "print(\"\\nAverage pooling result:\")\n",
    "print(pooled_avg.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7932597f",
   "metadata": {},
   "source": [
    "## 6. Complete CNN Architecture\n",
    "\n",
    "Now let's combine everything into a complete CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Block 1: Conv -> ReLU -> Pool\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 32x32 -> 16x16\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 16x16 -> 8x8\n",
    "        \n",
    "        # Block 3\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # 8x8 -> 4x4\n",
    "        \n",
    "        # Flatten: Convert 3D features to 1D\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiate and test\n",
    "model = SimpleCNN(num_classes=10)\n",
    "x = torch.randn(4, 3, 32, 32)  # Batch of 4 images\n",
    "output = model(x)\n",
    "\n",
    "print(f'Model output shape: {output.shape}')  # (4, 10) - 10 class scores\n",
    "print(f'\\nModel architecture:')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "param_count",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,}')\n",
    "\n",
    "# Layer-wise parameter count\n",
    "print(\"\\nParameters per layer:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'{name:20s}: {param.numel():>8,} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimension_tracking",
   "metadata": {},
   "source": [
    "## 7. Tracking Dimensions Through Network\n",
    "\n",
    "Understanding how dimensions change is crucial for debugging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "track_dims",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerboseCNN(nn.Module):\n",
    "    \"\"\"CNN that prints shape after each operation\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"Input: {x.shape}\")\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        print(f\"After conv1 + ReLU: {x.shape}\")\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        print(f\"After pool: {x.shape}\")\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        print(f\"After conv2 + ReLU: {x.shape}\")\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        print(f\"After pool: {x.shape}\")\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(f\"After flatten: {x.shape}\")\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        print(f\"After fc: {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "verbose_model = VerboseCNN()\n",
    "test_input = torch.randn(2, 3, 32, 32)\n",
    "_ = verbose_model(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c786b63b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "- ✓ Convolutional layers detect features\n",
    "- ✓ Padding prevents dimension shrinking\n",
    "- ✓ How to calculate output dimensions\n",
    "- ✓ Activation functions add non-linearity\n",
    "- ✓ Pooling layers downsample features\n",
    "- ✓ Complete CNN architecture\n",
    "\n",
    "**Key Takeaway:** CNNs learn hierarchical features automatically!\n",
    "\n",
    "**Next:** Training CNNs on real data (CIFAR-10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
