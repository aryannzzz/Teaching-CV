{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "344658b3",
   "metadata": {},
   "source": [
    "# Day 3: Training CNNs on CIFAR-10\n",
    "## CV Bootcamp 2024\n",
    "\n",
    "Train your first CNN from scratch and learn to debug training issues!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa1f58",
   "metadata": {},
   "source": [
    "## 1. Prepare CIFAR-10 Dataset\n",
    "\n",
    "CIFAR-10: 60,000 32x32 color images in 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae036ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False,\n",
    "                               download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')\n",
    "\n",
    "# Classes\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "def show_images(images, labels, classes, num=8):\n",
    "    fig, axes = plt.subplots(1, num, figsize=(15, 2))\n",
    "    for i in range(num):\n",
    "        img = images[i].numpy().transpose(1, 2, 0)\n",
    "        img = img * 0.5 + 0.5  # Denormalize\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(classes[labels[i]])\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "show_images(images, labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9207ab",
   "metadata": {},
   "source": [
    "## 2. Define CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae61b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CIFAR10CNN().to(device)\n",
    "print(model)\n",
    "print(f'\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sanity_check",
   "metadata": {},
   "source": [
    "## 3. Sanity Check: Overfit One Batch\n",
    "\n",
    "**Before training on full dataset, verify the model can learn by overfitting one batch!**\n",
    "\n",
    "This tests:\n",
    "- Model architecture is correct\n",
    "- Gradients flow properly\n",
    "- Loss function works\n",
    "- Optimizer is configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sanity_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one batch\n",
    "test_images, test_labels = next(iter(train_loader))\n",
    "test_images, test_labels = test_images.to(device), test_labels.to(device)\n",
    "\n",
    "# Simple model for sanity check\n",
    "sanity_model = CIFAR10CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(sanity_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Sanity Check: Overfitting one batch...\")\n",
    "print(\"Loss should decrease to near 0\\n\")\n",
    "\n",
    "for i in range(100):\n",
    "    outputs = sanity_model(test_images)\n",
    "    loss = criterion(outputs, test_labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        acc = (predicted == test_labels).sum().item() / test_labels.size(0)\n",
    "        print(f'Iteration {i:3d}: Loss = {loss.item():.4f}, Acc = {acc*100:.1f}%')\n",
    "\n",
    "print(\"\\nâœ“ Sanity check passed! Model can learn.\" if loss.item() < 0.1 else \"âœ— Something wrong - loss should be near 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd47058",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7240f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize model for actual training\n",
    "model = CIFAR10CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accs.append(epoch_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1} Summary: Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostics",
   "metadata": {},
   "source": [
    "## 5. Training Diagnostics\n",
    "\n",
    "### How to Read Your Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dffa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(train_losses, marker='o')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(train_accs, marker='o', color='green')\n",
    "ax2.set_title('Training Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Diagnose training\n",
    "print(\"\\nðŸ“Š Training Diagnostics:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if train_losses[-1] < train_losses[0] * 0.5:\n",
    "    print(\"âœ“ Loss is decreasing - Good!\")\n",
    "else:\n",
    "    print(\"âš  Loss not decreasing enough - Try:\")\n",
    "    print(\"  - Lower learning rate\")\n",
    "    print(\"  - Check data normalization\")\n",
    "\n",
    "if train_accs[-1] > 60:\n",
    "    print(\"âœ“ Accuracy is improving - Good!\")\n",
    "else:\n",
    "    print(\"âš  Low accuracy - Try:\")\n",
    "    print(\"  - Train more epochs\")\n",
    "    print(\"  - Increase model capacity\")\n",
    "\n",
    "if all(train_losses[i] > train_losses[i+1] for i in range(len(train_losses)-1)):\n",
    "    print(\"âœ“ Smooth decrease - Training is stable\")\n",
    "else:\n",
    "    print(\"âš  Unstable training - Consider:\")\n",
    "    print(\"  - Reduce learning rate\")\n",
    "    print(\"  - Use learning rate scheduling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8133c631",
   "metadata": {},
   "source": [
    "## 6. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490cc5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "class_correct = [0] * 10\n",
    "class_total = [0] * 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Per-class accuracy\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Overall Test Accuracy: {test_accuracy:.2f}%\\n')\n",
    "\n",
    "print('Per-class Accuracy:')\n",
    "for i in range(10):\n",
    "    acc = 100 * class_correct[i] / class_total[i]\n",
    "    print(f'{classes[i]:10s}: {acc:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predictions",
   "metadata": {},
   "source": [
    "## 7. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_preds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# Make predictions\n",
    "outputs = model(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Show results\n",
    "images = images.cpu()\n",
    "labels = labels.cpu()\n",
    "predicted = predicted.cpu()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(8):\n",
    "    img = images[i].numpy().transpose(1, 2, 0)\n",
    "    img = img * 0.5 + 0.5  # Denormalize\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    \n",
    "    true_label = classes[labels[i]]\n",
    "    pred_label = classes[predicted[i]]\n",
    "    \n",
    "    color = 'green' if labels[i] == predicted[i] else 'red'\n",
    "    axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}', color=color)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debugging",
   "metadata": {},
   "source": [
    "## 8. Common Problems & Solutions\n",
    "\n",
    "### Problem: Loss is NaN\n",
    "```python\n",
    "# Solutions:\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Reduce LR\n",
    "# Check for inf/nan in data\n",
    "torch.isnan(images).any()  # Should be False\n",
    "```\n",
    "\n",
    "### Problem: Loss not decreasing\n",
    "```python\n",
    "# Check learning rate\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(param_group['lr'])  # Should be 0.001-0.0001\n",
    "\n",
    "# Check gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f'{name}: {param.grad.norm():.4f}')\n",
    "```\n",
    "\n",
    "### Problem: Training acc high, test acc low (Overfitting)\n",
    "```python\n",
    "# Add data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Increase dropout\n",
    "self.dropout = nn.Dropout(0.7)  # Was 0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_model",
   "metadata": {},
   "source": [
    "## 9. Save Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'epoch': num_epochs,\n",
    "}, 'cifar10_cnn.pth')\n",
    "\n",
    "print(\"Model saved to 'cifar10_cnn.pth'\")\n",
    "\n",
    "# Load model later\n",
    "# checkpoint = torch.load('cifar10_cnn.pth')\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa3234d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "- âœ“ Preparing CIFAR-10 dataset\n",
    "- âœ“ Building CNN architecture\n",
    "- âœ“ Sanity checking before full training\n",
    "- âœ“ Training loop implementation\n",
    "- âœ“ Model evaluation\n",
    "- âœ“ Visualizing training progress\n",
    "- âœ“ Debugging common issues\n",
    "- âœ“ Saving and loading models\n",
    "\n",
    "**Congratulations! You trained your first CNN!**\n",
    "\n",
    "**Next:** Transfer Learning for even better results with less data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
